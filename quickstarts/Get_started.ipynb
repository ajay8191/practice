{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtVOlmDSHmh4"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC3bJQJcUKFk",
        "outputId": "0002a8a7-5a86-42d0-fa16-7a39206a4128"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Upload the file using the API\n",
        "file_upload = client.files.upload(file=text_path)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        file_upload,\n",
        "        \"Can you give me a summary of this information please?\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9r9Ggw012g9c"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLV19RrMUlaw"
      },
      "source": [
        "### Upload a PDF file\n",
        "\n",
        "This PDF page is an article titled [Smoothly editing material properties of objects](https://research.google/blog/smoothly-editing-material-properties-of-objects-with-text-to-image-models-and-synthetic-data/) with text-to-image models and synthetic data available on the Google Research Blog."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0BfhLDFWfCS",
        "outputId": "17e587bc-7f8c-4494-b793-2684e559ce9c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6695391"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Prepare the file to be uploaded\n",
        "PDF = \"https://storage.googleapis.com/generativeai-downloads/data/Smoothly%20editing%20material%20properties%20of%20objects%20with%20text-to-image%20models%20and%20synthetic%20data.pdf\"  # @param {type: \"string\"}\n",
        "pdf_bytes = requests.get(PDF).content\n",
        "\n",
        "pdf_path = pathlib.Path('article.pdf')\n",
        "pdf_path.write_bytes(pdf_bytes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH2h2WDVWptt",
        "outputId": "3037ccc3-3d5d-484b-daa6-753ff8a88939"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Here is a summary of the article as a bulleted list:\n\n*   The article presents a method called \"Alchemist\" for smoothly and parametrically editing the material properties (like color, shininess, or transparency) of objects in photographs.\n*   The goal is to achieve photorealistic edits while preserving the object's shape and the original scene lighting.\n*   Existing methods, such as intrinsic image decomposition or direct text-to-image model editing, struggle with the ambiguity of material properties or fail to disentangle material from shape.\n*   The proposed method leverages the photorealistic capabilities of generative text-to-image (T2I) models by fine-tuning them on a large synthetic dataset.\n*   The synthetic dataset is created by rendering 3D models of objects with varying material attributes and systematically changing one attribute at a time (e.g., roughness, transparency) according to a scalar \"edit strength\" value.\n*   A modified Stable Diffusion 1.5 model is trained to accept an input image, an edit instruction, and the desired edit strength, learning to translate these inputs into an output image with the edited material property.\n*   The model successfully generalizes to real-world images, producing photorealistic material changes while largely maintaining the original object's geometry and lighting.\n*   It can realistically render complex effects like backgrounds visible through transparent objects and caustic lighting effects.\n*   A user study showed that the method's edits were significantly more photorealistic and preferred over a baseline method (InstructPix2Pix).\n*   Potential applications include creating product mock-ups and enabling 3D consistent material editing when combined with techniques like NeRF.\n*   The research was presented in a paper at CVPR 2024.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Upload the file using the API\n",
        "file_upload = client.files.upload(file=pdf_path)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        file_upload,\n",
        "        \"Can you summarize this file as a bulleted list?\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NWO1moe9fx-"
      },
      "source": [
        "### Upload an audio file\n",
        "\n",
        "In this case, you'll use a [sound recording](https://www.jfklibrary.org/asset-viewer/archives/jfkwha-006) of President John F. Kennedy’s 1961 State of the Union address."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCSuGd9i9fEB",
        "outputId": "3a5b62a9-e26c-4804-dc31-fc47f4419f1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "41762063"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Prepare the file to be uploaded\n",
        "AUDIO = \"https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3\"  # @param {type: \"string\"}\n",
        "audio_bytes = requests.get(AUDIO).content\n",
        "\n",
        "audio_path = pathlib.Path('audio.mp3')\n",
        "audio_path.write_bytes(audio_bytes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wjKO0eI9yps",
        "outputId": "e1b59fb1-9eb0-4d54-8f45-a246723cc7a3"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "This audio is President John F. Kennedy's first State of the Union address, delivered on January 30, 1961.\n\nIn the speech, Kennedy provides a frank assessment of the nation's situation, highlighting both domestic and international challenges. Domestically, he details a struggling economy with high unemployment, low growth, and issues in housing, education, and healthcare, proposing immediate actions to address them. Internationally, he discusses the concerning balance of payments deficit and the growing threats posed by the Cold War and communism in various regions (Asia, Africa, Latin America). He calls for strengthening military, economic, and diplomatic capabilities, emphasizing the need for robust alliances, international cooperation (including in science and space), and a reformed, more decisive public service. The speech stresses the importance of facing difficulties realistically, preparing for future challenges, and requires dedication from all citizens to secure freedom and progress worldwide.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Upload the file using the API\n",
        "file_upload = client.files.upload(file=audio_path)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        file_upload,\n",
        "        \"Listen carefully to the following audio file. Provide a brief summary\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdUjkIQP-G_i"
      },
      "source": [
        "### Upload a video file\n",
        "\n",
        "In this case, you'll use a short clip of [Big Buck Bunny](https://peach.blender.org/about/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9ohtLxU-SFE",
        "outputId": "6dde6dc2-5769-46ad-9115-01453df5bb57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-18 12:09:07--  https://download.blender.org/peach/bigbuckbunny_movies/BigBuckBunny_320x180.mp4\n",
            "Resolving download.blender.org (download.blender.org)... 172.67.14.163, 104.22.65.163, 104.22.64.163, ...\n",
            "Connecting to download.blender.org (download.blender.org)|172.67.14.163|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64657027 (62M) [video/mp4]\n",
            "Saving to: ‘BigBuckBunny_320x180.mp4’\n",
            "\n",
            "BigBuckBunny_320x18 100%[===================>]  61.66M   141MB/s    in 0.4s    \n",
            "\n",
            "2025-04-18 12:09:08 (141 MB/s) - ‘BigBuckBunny_320x180.mp4’ saved [64657027/64657027]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the video file\n",
        "VIDEO_URL = \"https://download.blender.org/peach/bigbuckbunny_movies/BigBuckBunny_320x180.mp4\"  # @param {type: \"string\"}\n",
        "video_file_name = \"BigBuckBunny_320x180.mp4\"\n",
        "!wget -O {video_file_name} $VIDEO_URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyFVXPspS5GF"
      },
      "source": [
        "Let's start by uploading the video file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY1WlxMk-0Uy",
        "outputId": "942770b2-0bd5-4ac3-e1ec-62758eae8b10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed upload: https://generativelanguage.googleapis.com/v1beta/files/prqn913jn9t8\n"
          ]
        }
      ],
      "source": [
        "# Upload the file using the API\n",
        "video_file = client.files.upload(file=video_file_name)\n",
        "print(f\"Completed upload: {video_file.uri}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yRG9BPXS65b"
      },
      "source": [
        "The state of the video is important. The video must finish processing, so do check the state. Once the state of the video is `ACTIVE`, you are able to pass it into `generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEk4P3fK_OcJ",
        "outputId": "3fba4dfc-9004-4281-950a-38a891b146d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for video to be processed.\n",
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/prqn913jn9t8\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Check the file processing state\n",
        "while video_file.state == \"PROCESSING\":\n",
        "    print('Waiting for video to be processed.')\n",
        "    time.sleep(10)\n",
        "    video_file = client.files.get(name=video_file.name)\n",
        "\n",
        "if video_file.state == \"FAILED\":\n",
        "  raise ValueError(video_file.state)\n",
        "print(f'Video processing complete: ' + video_file.uri)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMz9GIuvAiCO",
        "outputId": "9a182589-a17f-487d-a309-dd0b072221c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FileState.ACTIVE\n"
          ]
        }
      ],
      "source": [
        "print(video_file.state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX82TyGL-e2O",
        "outputId": "316d0dee-1b63-4b9c-9541-f35f984c80b5"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "The video is a clip from the open-source animated short film \"Big Buck Bunny\" (produced by the Blender Foundation). It opens with a peaceful pastoral scene: rolling green hills, scattered trees (including pine and deciduous), rocks, flowers, and a stream, under a bright sky with fluffy pink clouds.\n\nA small, plump, grey bird is perched on a branch, yawning and stretching, but is soon knocked off.\n\nThe camera then focuses on a large burrow entrance under a tree root, where a very large, fluffy, grey rabbit is sleeping. It wakes up, stretches, emerges from the burrow, and smiles contentedly at the sunny morning.\n\nThe rabbit enjoys the day, sniffing large white flowers and watching a beautiful pink butterfly land on its head. An apple falls from a tree, but the rabbit's attention is drawn back to the butterfly.\n\nHiding behind a tree root are three smaller rodent characters: two squirrels (one brown, one reddish-brown and spikier) and a grey chinchilla/hamster, all looking mischievous. The chinchilla holds a nut.\n\nThe squirrels begin to torment the rabbit by throwing small objects at it (rocks, nuts, and spiky chestnuts). The rabbit is initially startled and confused, but quickly becomes annoyed and then angry.\n\nDriven by vengeance, Big Buck Bunny decides to retaliate. He prepares by sharpening a stick with a rock and creating a large spear using a vine as a bowstring. He takes aim at the squirrels hiding behind a tree and shoots the spear, which punctures the tree trunk.\n\nUndeterred, the squirrels continue their harassment. Big Buck then sets up a trap: a series of sharpened sticks concealed under leaves on the ground, connected by a vine which he pulls taut like a tripwire.\n\nThe angry flying squirrel tries to knock a peach from a tree but ends up knocking it towards the stakes, where it gets impaled. Big Buck then catches the flying squirrel.\n\nIn the final scene before the credits, Big Buck Bunny is seen happily flying the terrified flying squirrel like a kite.\n\nThe credits roll, featuring brief animated appearances of the chinchilla and the red squirrel interacting with the text, and finally the little bird flying the flying squirrel (still as a kite) past the credits.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ask Gemini about the video\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        video_file,\n",
        "        \"Describe this video.\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65_qu3UsM8_M"
      },
      "source": [
        "### Process a YouTube link"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXNlrAsZR7bB"
      },
      "source": [
        "For YouTube links, you don't need to explicitly upload the video file content, but you do need to explicitly declare the video URL you want the model to process as part of the `contents` of the request. For more information see the [vision](https://ai.google.dev/gemini-api/docs/vision?lang=python#youtube) documentation including the features and limits.\n",
        "\n",
        "> **Note:** You are only able to submit up to one YouTube link per `generate_content` request.\n",
        "\n",
        "> **Note:** YouTube links included as part of the text input won't being processed in the request, an can lead to incorrect responses. You must explicitly the URL using the `file_uri` argument of `FileData`.\n",
        "\n",
        "The following example shows how you can use the model to summarize the video. In this case use a summary video of [Google I/O 2024](\"https://www.youtube.com/watch?v=WsEQjeZoEng\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcqXUrYSTrLQ",
        "outputId": "70e20872-20e1-4903-9ae5-eebf18298d85"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Based on the video, here is a summary of the Google I/O 2024 keynote:\n\nThe keynote highlights Google's progress in the \"Gemini era,\" integrating their multimodal AI models across their products and introducing new capabilities and models. Key announcements and features include:\n\n1.  **Gemini Integration:** Gemini is now integrated into all of Google's 2 billion user products, enhancing existing features.\n2.  **Gemini 1.5 Pro in Workspace:** Available today in Workspace Labs, it can summarize long emails and potentially other documents. It can also summarize recorded Google Meet meetings.\n3.  **Gemini in Google Photos:** Enables deeper search capabilities, allowing users to find specific memories or track progress over time by understanding the content within photos and videos.\n4.  **Expanded Context Window:** Gemini 1.5 Pro's context window is expanded to 2 million tokens, allowing it to process much larger amounts of information simultaneously (e.g., summarizing very long documents or videos).\n5.  **Project Astra:** A prototype for a universal AI agent that is truly helpful in everyday life. Demos show the agent understanding real-time visual and audio input to explain code, remember object locations, and even suggest creative ideas (like a band name for a dog and a toy).\n6.  **Gemini 1.5 Flash:** A new, lighter-weight, faster, and more cost-efficient multimodal model designed for scaling, while still retaining strong reasoning and long-context capabilities.\n7.  **Veo:** A new, highly capable generative video model that creates high-quality 1080p videos from text, image, and video prompts.\n8.  **Trillium TPUs:** The 6th generation of Google's custom chips for AI/ML, delivering a 4.7x improvement in compute performance per chip over the previous generation.\n9.  **Generative AI in Google Search:** AI Overviews are becoming more powerful, able to handle complex, multi-part questions and provide quick answers and summaries. This is coming to over 1 billion people by the end of the year.\n10. **Google Lens Integration:** Soon, users can ask questions about a video by pointing Google Lens at it, getting relevant information instantly (e.g., troubleshooting a turntable based on visual input).\n11. **Gems:** Customizable personal AI experts within Gemini, available for Gemini Advanced subscribers. Users can create specific assistants for their needs by providing instructions, which can then handle complex tasks and answer questions across multiple uploaded files (up to 1500 pages per PDF or multiple files for project insights). Gemini Advanced offers a 1 million token context window for this.\n12. **AI in Android:** Gemini is being reimagined at the core of Android to be more context-aware, anticipating user needs and providing helpful suggestions in the moment. Gemini Nano with Multimodality will enable the phone to understand the world through sight, sound, and spoken language.\n13. **Gemma & PaliGemma:** Expansion of the open model family. PaliGemma is the first vision-language open model, available now.\n14. **Gemma 2:** The next generation of Gemma, including a new 27 billion parameter model, will be available in June for driving AI innovation responsibly.\n15. **LearnLM:** A new family of models based on Gemini and fine-tuned for learning. A new feature in YouTube uses LearnLM to make educational videos more interactive, allowing users to ask clarifying questions, get explanations, and take quizzes.\n16. **Responsible AI:** Google emphasizes its commitment to building AI responsibly through practices like red teaming to identify and address potential risks while maximizing benefits for society.\n\nOverall, the keynote showcases Google's focus on making AI, particularly through the Gemini family of models, more powerful, multimodal, context-aware, and helpful across its platforms and products, while also emphasizing responsible development.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents= types.Content(\n",
        "        parts=[\n",
        "            types.Part(text=\"Summarize this video.\"),\n",
        "            types.Part(\n",
        "                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=WsEQjeZoEng')\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTgeR3_9wN5J"
      },
      "source": [
        "## Use context caching\n",
        "\n",
        "[Context caching](https://ai.google.dev/gemini-api/docs/caching?lang=python) lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model.\n",
        "\n",
        "Context caching is only available for stable models with fixed versions (for example, `gemini-1.5-flash-002`). You must include the version postfix (for example, the `-002` in `gemini-1.5-flash-002`). You can find more caching examples [here](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Caching.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tgl2gzmuwQXz"
      },
      "source": [
        "#### Create a cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2Jb0gaiwOVi"
      },
      "outputs": [],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
        "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
        "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
        "\"\"\"\n",
        "\n",
        "urls = [\n",
        "    'https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf',\n",
        "    \"https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrZ64o5Sydm_",
        "outputId": "e396077f-a340-4724-c333-fef797c1f8ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7228817"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download files\n",
        "pdf_bytes = requests.get(urls[0]).content\n",
        "pdf_path = pathlib.Path('2312.11805v3.pdf')\n",
        "pdf_path.write_bytes(pdf_bytes)\n",
        "\n",
        "pdf_bytes = requests.get(urls[1]).content\n",
        "pdf_path = pathlib.Path('2403.05530.pdf')\n",
        "pdf_path.write_bytes(pdf_bytes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrylX7r3w2bF"
      },
      "outputs": [],
      "source": [
        "# Upload the PDFs using the File API\n",
        "uploaded_pdfs = []\n",
        "uploaded_pdfs.append(client.files.upload(file='2312.11805v3.pdf'))\n",
        "uploaded_pdfs.append(client.files.upload(file='2403.05530.pdf'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MBsaipow7m5",
        "outputId": "b8983503-76b2-44c5-cd8e-7a032dc4cf89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CachedContent(name='cachedContents/ql5fbzexj5rl', display_name='research papers', model='models/gemini-2.5-flash-preview-04-17', create_time=datetime.datetime(2025, 4, 18, 12, 10, 43, 598484, tzinfo=TzInfo(UTC)), update_time=datetime.datetime(2025, 4, 18, 12, 10, 43, 598484, tzinfo=TzInfo(UTC)), expire_time=datetime.datetime(2025, 4, 18, 13, 10, 42, 12326, tzinfo=TzInfo(UTC)), usage_metadata=CachedContentUsageMetadata(audio_duration_seconds=None, image_count=None, text_count=None, total_token_count=43167, video_duration_seconds=None))"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a cache with a 60 minute TTL\n",
        "cached_content = client.caches.create(\n",
        "    model=MODEL_ID,\n",
        "    config=types.CreateCachedContentConfig(\n",
        "      display_name='research papers', # used to identify the cache\n",
        "      system_instruction=system_instruction,\n",
        "      contents=uploaded_pdfs,\n",
        "      ttl=\"3600s\",\n",
        "  )\n",
        ")\n",
        "\n",
        "cached_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2870527e1c84"
      },
      "source": [
        "#### Listing available cache objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3be7c2339be3",
        "outputId": "cde4ca7d-a8b5-452e-dd01-2ca166d7bde7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name='cachedContents/ql5fbzexj5rl' display_name='research papers' model='models/gemini-2.5-flash-preview-04-17' create_time=datetime.datetime(2025, 4, 18, 12, 10, 43, 598484, tzinfo=TzInfo(UTC)) update_time=datetime.datetime(2025, 4, 18, 12, 10, 43, 598484, tzinfo=TzInfo(UTC)) expire_time=datetime.datetime(2025, 4, 18, 13, 10, 42, 12326, tzinfo=TzInfo(UTC)) usage_metadata=CachedContentUsageMetadata(audio_duration_seconds=None, image_count=None, text_count=None, total_token_count=43167, video_duration_seconds=None)\n"
          ]
        }
      ],
      "source": [
        "for cache in client.caches.list():\n",
        "  print(cache)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKgCRRXfwU_m"
      },
      "source": [
        "#### Use a cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Qo7-sU2w92j",
        "outputId": "e71c0420-b547-4db7-86c7-063bb59958d0"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Both research papers share the goal of developing and presenting the Gemini family of highly capable multimodal models. These models aim to understand and reason across image, audio, video, and text data.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "  model=MODEL_ID,\n",
        "  contents=\"What is the research goal shared by these research papers?\",\n",
        "  config=types.GenerateContentConfig(cached_content=cached_content.name)\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpJIA5zmx8Vy"
      },
      "outputs": [],
      "source": [
        "TEXT_EMBEDDING_MODEL_ID = \"gemini-embedding-exp-03-07\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kJSYsmYMIuNd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Mfk6YY3G5kqp"
      ],
      "name": "Get_started.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}